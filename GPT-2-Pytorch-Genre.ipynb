{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GPT-2_Generateur-histoire-par-genre.ipynb","provenance":[{"file_id":"1l8rqAB4KMzAIDhqRN_DzHaxukDsEhrTX","timestamp":1636958763142}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"_Rb1BSLugcZn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636965248720,"user_tz":-60,"elapsed":8304,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}},"outputId":"3bf26419-7808-40ec-8cdd-d46b0c88ecac"},"source":["# First upload the training and evaluation files to this runtime (Press connect if needed)\n","!pip install transformers torch"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 40.7 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 48.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 6.9 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 33.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.3\n"]}]},{"cell_type":"code","metadata":{"id":"ltEwIIMDg9ex","executionInfo":{"status":"ok","timestamp":1636965284714,"user_tz":-60,"elapsed":29114,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}}},"source":["import logging\n","import math\n","import os\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","from transformers import (\n","    CONFIG_MAPPING,\n","    MODEL_WITH_LM_HEAD_MAPPING,\n","    AutoConfig,\n","    GPT2LMHeadModel,\n","    AutoTokenizer,\n","    DataCollatorForLanguageModeling,\n","    LineByLineTextDataset,\n","    PreTrainedTokenizer,\n","    TextDataset,\n","    Trainer,\n","    TrainingArguments,\n","    set_seed,\n",")\n","\n","# Setup logging\n","logger = logging.getLogger(__name__)\n","\n","# Get access to model types and model configs to select GPT2 model and config\n","MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hIVlxeXhXy5l","executionInfo":{"status":"ok","timestamp":1636965307709,"user_tz":-60,"elapsed":18364,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}},"outputId":"a0cbba44-0468-480a-c583-1e74fb11936e"},"source":["from google.colab import drive # Se connecter a google drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KfGicxA_X3oG","executionInfo":{"status":"ok","timestamp":1636965309777,"user_tz":-60,"elapsed":437,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}},"outputId":"89300a34-8421-4a3c-b208-6573ac648550"},"source":["!nvidia-smi"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Nov 15 08:35:09 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"Uz7YCuDrX-WZ","executionInfo":{"status":"ok","timestamp":1636965316004,"user_tz":-60,"elapsed":3553,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}}},"source":["# Importer le dataset \n","validation_data = open(\"/content/drive/My Drive/projet-TIA/data-stories/6_genre_eval_data.txt\", 'rt', encoding='utf-8').read()\n","training_data = open(\"/content/drive/My Drive/projet-TIA/data-stories/6_genre_clean_training_data.txt\", 'rt', encoding='utf-8').read()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"IL0YR9WMhG0v","executionInfo":{"status":"ok","timestamp":1636965320445,"user_tz":-60,"elapsed":196,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}}},"source":["@dataclass\n","class ModelArguments:\n","    \"\"\"\n","   Arguments relatifs au modèle/configuration/tokenizer que nous allons affiner ou former à partir de zéro.\n","    \"\"\"\n","\n","    model_name_or_path: Optional[str] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"Le point de contrôle du modèle pour l'initialisation des poids. Ne laissez aucun si vous souhaitez entraîner un modèle à partir de zéro.\"\n","        },\n","    )\n","    model_type: Optional[str] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"Si vous vous entraînez à partir de zéro, transmettez un type de modèle de la liste :\" \n","            + \", \".join(MODEL_TYPES)\n","        },\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"Où voulez-vous stocker les modèles pré-entraînés téléchargés à partir de s3\"\n","        },\n","    )\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"fX9aWWf1hL4X","executionInfo":{"status":"ok","timestamp":1636965328253,"user_tz":-60,"elapsed":176,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}}},"source":["@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments relatifs aux données que nous allons saisir dans notre modèle pour la formation et l'évaluation\n","    \"\"\"\n","\n","    train_data_file: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Le fichier de données d'entraînement d'entrée (un fichier texte).\"}\n","    )\n","    eval_data_file: Optional[str] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"Un fichier de données d'évaluation d'entrée facultatif pour évaluer la perplexité sur (un fichier texte).\"\n","        },\n","    )\n","    line_by_line: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Si des lignes de texte distinctes dans l'ensemble de données doivent être traitées comme des séquences distinctes.\"\n","        },\n","    )\n","\n","    mlm: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Entraînez-vous avec une perte de modélisation du langage masqué au lieu de la modélisation du langage.\"\n","        },\n","    )\n","\n","    block_size: int = field(\n","        default=-1,\n","        metadata={\n","            \"help\": \"Longueur de séquence d'entrée facultative après la tokenisation.\"\n","            \"L'ensemble de données d'entraînement sera tronqué en bloc de cette taille pour l'entraînement.\"\n","            \"Par défaut, la longueur d'entrée maximale du modèle pour les entrées de phrase unique (prenez en compte les jetons spéciaux).\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False,\n","        metadata={\"help\": \"Ecraser les ensembles d'entraînement et d'évaluation mis en cache\"},\n","    )\n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Od_Gxsz1hZvV","executionInfo":{"status":"ok","timestamp":1636965336220,"user_tz":-60,"elapsed":208,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}}},"source":["# Create LineByLineDataset from Movie Plots text file\n","def get_dataset(\n","    args: DataTrainingArguments, tokenizer: PreTrainedTokenizer, evaluate=False\n","):\n","    file_path = args.eval_data_file if evaluate else args.train_data_file\n","    if args.line_by_line:\n","        return LineByLineTextDataset(\n","            tokenizer=tokenizer, file_path=file_path, block_size=args.block_size\n","        )\n","    else:\n","        return TextDataset(\n","            tokenizer=tokenizer,\n","            file_path=file_path,\n","            block_size=args.block_size,\n","            overwrite_cache=args.overwrite_cache,\n","        )\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"AXE6Zq8-hikF","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1636965994704,"user_tz":-60,"elapsed":194,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}},"outputId":"f97a9fcd-d33b-42ff-a3f2-073f52a68a7b"},"source":["def main():\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"gpt2\", model_type=\"gpt2\"\n","    )\n","    data_args = DataTrainingArguments(\n","        train_data_file=training_data,\n","        eval_data_file=validation_data,\n","        line_by_line=True,\n","        block_size=512,\n","        overwrite_cache=True,\n","    )\n","    training_args = TrainingArguments(\n","        output_dir=\"/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint\",\n","        overwrite_output_dir=True,\n","        do_train=True,\n","        do_eval=True,\n","        evaluation_strategy='epoch',\n","        logging_steps=500,\n","        per_device_train_batch_size=4,\n","        num_train_epochs=3,\n","        save_total_limit=1,\n","        save_steps=1000,\n","    )\n","\n","    if data_args.eval_data_file is None and training_args.do_eval:\n","        raise ValueError(\n","            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n","            \"or remove the --do_eval argument.\"\n","        )\n","\n","    if (\n","        os.path.exists(training_args.output_dir)\n","        and os.listdir(training_args.output_dir)\n","        and training_args.do_train\n","        and not training_args.overwrite_output_dir\n","    ):\n","        raise ValueError(\n","            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n","        )\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        training_args.local_rank,\n","        training_args.device,\n","        training_args.n_gpu,\n","        bool(training_args.local_rank != -1),\n","        training_args.fp16,\n","    )\n","    logger.info(\"Training/evaluation parameters %s\", training_args)\n","\n","    # Set seed for deterministic training runs\n","    set_seed(training_args.seed)\n","\n","\n","    config = AutoConfig.from_pretrained(\n","        model_args.model_name_or_path, cache_dir=model_args.cache_dir\n","    )\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.model_name_or_path, cache_dir=model_args.cache_dir\n","    )\n","\n","    model = GPT2LMHeadModel.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","    )\n","\n","    special_tokens_dict = {\n","        \"bos_token\": \"<BOS>\",\n","        \"eos_token\": \"<EOS>\",\n","        \"pad_token\": \"<PAD>\",\n","        \"additional_special_tokens\": [\n","            \"<superhero>\",\n","            \"<action>\",\n","            \"<drama>\",\n","            \"<thriller>\",\n","            \"<horror>\",\n","            \"<sci_fi>\",\n","        ],\n","    }\n","\n","    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","    if data_args.block_size <= 0: \n","      # If block_size <= 0, set it to max. possible value allowed by model\n","        data_args.block_size = tokenizer.model_max_length\n","    else:\n","        data_args.block_size = min(data_args.block_size, tokenizer.model_max_length)\n","\n","    # Get datasets\n","\n","    train_dataset = (\n","        get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\n","    )\n","    eval_dataset = (\n","        get_dataset(data_args, tokenizer=tokenizer, evaluate=True)\n","        if training_args.do_eval\n","        else None\n","    )\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer,\n","        mlm=data_args.mlm,\n","    )\n","\n","    # Initialize our Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        data_collator=data_collator,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        prediction_loss_only=True,\n","    )\n","\n","    # Training\n","    try:\n","      if training_args.do_train:\n","          model_path = (\n","              model_args.model_name_or_path\n","              if model_args.model_name_or_path is not None\n","              and os.path.isdir(model_args.model_name_or_path)\n","              else None\n","          )\n","          trainer.train(model_path=model_path)\n","          trainer.save_model()\n","          tokenizer.save_pretrained(training_args.output_dir)\n","    except KeyboardInterrupt:\n","      print(\"Saving model that was in the middle of training\")\n","      trainer.save_model()\n","      tokenizer.save_pretrained(training_args.output_dir)\n","      return\n","\n","    # Evaluation\n","    results = {}\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","\n","        eval_output = trainer.evaluate()\n","\n","        perplexity = math.exp(eval_output[\"eval_loss\"])\n","        result = {\"perplexity\": perplexity}\n","\n","        output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n","        if trainer.is_world_master():\n","            with open(output_eval_file, \"w\") as writer:\n","                logger.info(\"***** Eval results *****\")\n","                for key in sorted(result.keys()):\n","                    logger.info(\"  %s = %s\", key, str(result[key]))\n","                    writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","\n","        results.update(result)\n","\n","    return results\n"],"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"lX2E71vZz385","outputId":"33ac277d-2074-40bd-ca0d-48d67ec62c22"},"source":["main()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["11/15/2021 08:56:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","11/15/2021 08:56:08 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.EPOCH,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint/runs/Nov15_08-56-08_19f1934938df,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3,\n","output_dir=/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=4,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint,\n","save_on_each_node=False,\n","save_steps=1000,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=1,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"id":"RdzdlR4ciaZ6","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"133802c0-7656-4a45-fe99-4ce8c0f69fad"},"source":["# Press the Run Cell button to the left to start training\n","if __name__ == \"__main__\":\n","    main()\n","# To stop training and save model, press the same Run Cell button (now, it is the Interrupt Execution button)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["11/15/2021 08:46:35 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","11/15/2021 08:46:35 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.EPOCH,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint/runs/Nov15_08-46-35_19f1934938df,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3,\n","output_dir=/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=4,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint,\n","save_on_each_node=False,\n","save_steps=1000,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=1,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"id":"xQ4GPsSfQrDH"},"source":["# This cell is to style the Google Colab's output properly (Just blindly run this)\n","from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUoTHvmHHz43","colab":{"base_uri":"https://localhost:8080/","height":562},"executionInfo":{"status":"error","timestamp":1636966594330,"user_tz":-60,"elapsed":492,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}},"outputId":"73599fe9-53f2-4e39-9fc8-fbca3267b4a2"},"source":["# Run these cells for story generation\n","from transformers import pipeline, TextGenerationPipeline, GPT2LMHeadModel, AutoTokenizer\n","\n","\"\"\" Ci-dessous, mon point de contrôle modèle est commenté. Vous pouvez remplacer votre point de contrôle\n","avec cela pour tester la génération d'histoires si votre point de contrôle ne s'est pas entraîné assez longtemps\n","\"\"\"\n","#checkpoint = \"pranavpsv/gpt2-genre-story-generator\"\n","checkpoint = \"/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint\"\n","\n","model = GPT2LMHeadModel.from_pretrained(checkpoint)\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","story_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n","# The format for input_prompt: \"<BOS> <genre> Optional text...\"\n","# Supported genres: superhero, sci_fi, horror, thriller, action, drama"],"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["404 Client Error: Not Found for url: https://huggingface.co//content/drive/My%20Drive/projet-TIA/checkpoint/story_generator_checkpoint/resolve/main/config.json\n"]},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1662\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1663\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1664\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co//content/drive/My%20Drive/projet-TIA/checkpoint/story_generator_checkpoint/resolve/main/config.json","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-67a28ccc5667>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#checkpoint = \"/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mstory_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextGenerationPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m                 \u001b[0m_from_auto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_auto_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m                 \u001b[0m_from_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_pipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m             )\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \"\"\"\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             logger.warn(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"- or '{revision}' is a valid git identifier (branch name, a tag name, or a commit id) that exists for this model name as listed on its model page on 'https://huggingface.co/models'\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Can't load config for '/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint'. Make sure that:\n\n- '/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure '/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint' is not a path to a local directory with something else, in that case)\n\n- or '/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint' is the correct path to a directory containing a config.json file\n\n"]}]},{"cell_type":"code","metadata":{"id":"clXP7bNrPxja","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1598608678496,"user_tz":-120,"elapsed":6306,"user":{"displayName":"Pranav Vadrevu","photoUrl":"https://lh6.googleusercontent.com/-STjRKi70QFE/AAAAAAAAAAI/AAAAAAAACK4/8tBv3gsZ-Ro/s64/photo.jpg","userId":"00326039388269113607"}},"outputId":"cee1e04d-e13b-44ba-b096-7c1d91362ce7"},"source":["input_prompt = \"<BOS> <horror>\"\n","story = story_generator(input_prompt, max_length=75, do_sample=True,\n","               repetition_penalty=1.1, temperature=1.2, \n","               top_p=0.95, top_k=50)\n","print(story)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["08/28/2020 09:57:52 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"],"name":"stderr"},{"output_type":"stream","text":["[{'generated_text': '<BOS> <horror> Following a series of murders, paranormal researchers, led by Alex Riedlke (Bela Lugosi) believe there is a connection between the murders and paranormal activity in their community. Their research reveals a strange dark cycle that will keep on happening as people are killed by a strange force.  Riedlke and his assistants discover a secret society behind the bar'}]\n"],"name":"stdout"}]}]}