{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GPT-2_Generateur-histoire-par-genre.ipynb","provenance":[{"file_id":"1l8rqAB4KMzAIDhqRN_DzHaxukDsEhrTX","timestamp":1636958763142}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"_Rb1BSLugcZn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636965248720,"user_tz":-60,"elapsed":8304,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}},"outputId":"3bf26419-7808-40ec-8cdd-d46b0c88ecac"},"source":["# First upload the training and evaluation files to this runtime (Press connect if needed)\n","!pip install transformers torch"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.1 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 895 kB 40.7 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 596 kB 48.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59 kB 6.9 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.3 MB 33.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.3\n"]}]},{"cell_type":"code","metadata":{"id":"ltEwIIMDg9ex","executionInfo":{"status":"ok","timestamp":1636965284714,"user_tz":-60,"elapsed":29114,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}}},"source":["import logging\n","import math\n","import os\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","from transformers import (\n","    CONFIG_MAPPING,\n","    MODEL_WITH_LM_HEAD_MAPPING,\n","    AutoConfig,\n","    GPT2LMHeadModel,\n","    AutoTokenizer,\n","    DataCollatorForLanguageModeling,\n","    LineByLineTextDataset,\n","    PreTrainedTokenizer,\n","    TextDataset,\n","    Trainer,\n","    TrainingArguments,\n","    set_seed,\n",")\n","\n","# Setup logging\n","logger = logging.getLogger(__name__)\n","\n","# Get access to model types and model configs to select GPT2 model and config\n","MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hIVlxeXhXy5l","executionInfo":{"status":"ok","timestamp":1636965307709,"user_tz":-60,"elapsed":18364,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}},"outputId":"a0cbba44-0468-480a-c583-1e74fb11936e"},"source":["from google.colab import drive # Se connecter a google drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KfGicxA_X3oG","executionInfo":{"status":"ok","timestamp":1636965309777,"user_tz":-60,"elapsed":437,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}},"outputId":"89300a34-8421-4a3c-b208-6573ac648550"},"source":["!nvidia-smi"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Nov 15 08:35:09 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"Uz7YCuDrX-WZ","executionInfo":{"status":"ok","timestamp":1636965316004,"user_tz":-60,"elapsed":3553,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}}},"source":["# Importer le dataset \n","validation_data = open(\"/content/drive/My Drive/projet-TIA/data-stories/6_genre_eval_data.txt\", 'rt', encoding='utf-8').read()\n","training_data = open(\"/content/drive/My Drive/projet-TIA/data-stories/6_genre_clean_training_data.txt\", 'rt', encoding='utf-8').read()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"IL0YR9WMhG0v","executionInfo":{"status":"ok","timestamp":1636965320445,"user_tz":-60,"elapsed":196,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}}},"source":["@dataclass\n","class ModelArguments:\n","    \"\"\"\n","   Arguments relatifs au mod√®le/configuration/tokenizer que nous allons affiner ou former √† partir de z√©ro.\n","    \"\"\"\n","\n","    model_name_or_path: Optional[str] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"Le point de contr√¥le du mod√®le pour l'initialisation des poids. Ne laissez aucun si vous souhaitez entra√Æner un mod√®le √† partir de z√©ro.\"\n","        },\n","    )\n","    model_type: Optional[str] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"Si vous vous entra√Ænez √† partir de z√©ro, transmettez un type de mod√®le de la liste :\" \n","            + \", \".join(MODEL_TYPES)\n","        },\n","    )\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"O√π voulez-vous stocker les mod√®les pr√©-entra√Æn√©s t√©l√©charg√©s √† partir de s3\"\n","        },\n","    )\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"fX9aWWf1hL4X","executionInfo":{"status":"ok","timestamp":1636965328253,"user_tz":-60,"elapsed":176,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}}},"source":["@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments relatifs aux donn√©es que nous allons saisir dans notre mod√®le pour la formation et l'√©valuation\n","    \"\"\"\n","\n","    train_data_file: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Le fichier de donn√©es d'entra√Ænement d'entr√©e (un fichier texte).\"}\n","    )\n","    eval_data_file: Optional[str] = field(\n","        default=None,\n","        metadata={\n","            \"help\": \"Un fichier de donn√©es d'√©valuation d'entr√©e facultatif pour √©valuer la perplexit√© sur (un fichier texte).\"\n","        },\n","    )\n","    line_by_line: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Si des lignes de texte distinctes dans l'ensemble de donn√©es doivent √™tre trait√©es comme des s√©quences distinctes.\"\n","        },\n","    )\n","\n","    mlm: bool = field(\n","        default=False,\n","        metadata={\n","            \"help\": \"Entra√Ænez-vous avec une perte de mod√©lisation du langage masqu√© au lieu de la mod√©lisation du langage.\"\n","        },\n","    )\n","\n","    block_size: int = field(\n","        default=-1,\n","        metadata={\n","            \"help\": \"Longueur de s√©quence d'entr√©e facultative apr√®s la tokenisation.\"\n","            \"L'ensemble de donn√©es d'entra√Ænement sera tronqu√© en bloc de cette taille pour l'entra√Ænement.\"\n","            \"Par d√©faut, la longueur d'entr√©e maximale du mod√®le pour les entr√©es de phrase unique (prenez en compte les jetons sp√©ciaux).\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False,\n","        metadata={\"help\": \"Ecraser les ensembles d'entra√Ænement et d'√©valuation mis en cache\"},\n","    )\n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Od_Gxsz1hZvV","executionInfo":{"status":"ok","timestamp":1636965336220,"user_tz":-60,"elapsed":208,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}}},"source":["# Create LineByLineDataset from Movie Plots text file\n","def get_dataset(\n","    args: DataTrainingArguments, tokenizer: PreTrainedTokenizer, evaluate=False\n","):\n","    file_path = args.eval_data_file if evaluate else args.train_data_file\n","    if args.line_by_line:\n","        return LineByLineTextDataset(\n","            tokenizer=tokenizer, file_path=file_path, block_size=args.block_size\n","        )\n","    else:\n","        return TextDataset(\n","            tokenizer=tokenizer,\n","            file_path=file_path,\n","            block_size=args.block_size,\n","            overwrite_cache=args.overwrite_cache,\n","        )\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"AXE6Zq8-hikF","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1636965994704,"user_tz":-60,"elapsed":194,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}},"outputId":"f97a9fcd-d33b-42ff-a3f2-073f52a68a7b"},"source":["def main():\n","\n","    model_args = ModelArguments(\n","        model_name_or_path=\"gpt2\", model_type=\"gpt2\"\n","    )\n","    data_args = DataTrainingArguments(\n","        train_data_file=training_data,\n","        eval_data_file=validation_data,\n","        line_by_line=True,\n","        block_size=512,\n","        overwrite_cache=True,\n","    )\n","    training_args = TrainingArguments(\n","        output_dir=\"/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint\",\n","        overwrite_output_dir=True,\n","        do_train=True,\n","        do_eval=True,\n","        evaluation_strategy='epoch',\n","        logging_steps=500,\n","        per_device_train_batch_size=4,\n","        num_train_epochs=3,\n","        save_total_limit=1,\n","        save_steps=1000,\n","    )\n","\n","    if data_args.eval_data_file is None and training_args.do_eval:\n","        raise ValueError(\n","            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n","            \"or remove the --do_eval argument.\"\n","        )\n","\n","    if (\n","        os.path.exists(training_args.output_dir)\n","        and os.listdir(training_args.output_dir)\n","        and training_args.do_train\n","        and not training_args.overwrite_output_dir\n","    ):\n","        raise ValueError(\n","            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n","        )\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        training_args.local_rank,\n","        training_args.device,\n","        training_args.n_gpu,\n","        bool(training_args.local_rank != -1),\n","        training_args.fp16,\n","    )\n","    logger.info(\"Training/evaluation parameters %s\", training_args)\n","\n","    # Set seed for deterministic training runs\n","    set_seed(training_args.seed)\n","\n","\n","    config = AutoConfig.from_pretrained(\n","        model_args.model_name_or_path, cache_dir=model_args.cache_dir\n","    )\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_args.model_name_or_path, cache_dir=model_args.cache_dir\n","    )\n","\n","    model = GPT2LMHeadModel.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","    )\n","\n","    special_tokens_dict = {\n","        \"bos_token\": \"<BOS>\",\n","        \"eos_token\": \"<EOS>\",\n","        \"pad_token\": \"<PAD>\",\n","        \"additional_special_tokens\": [\n","            \"<superhero>\",\n","            \"<action>\",\n","            \"<drama>\",\n","            \"<thriller>\",\n","            \"<horror>\",\n","            \"<sci_fi>\",\n","        ],\n","    }\n","\n","    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","    if data_args.block_size <= 0: \n","      # If block_size <= 0, set it to max. possible value allowed by model\n","        data_args.block_size = tokenizer.model_max_length\n","    else:\n","        data_args.block_size = min(data_args.block_size, tokenizer.model_max_length)\n","\n","    # Get datasets\n","\n","    train_dataset = (\n","        get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\n","    )\n","    eval_dataset = (\n","        get_dataset(data_args, tokenizer=tokenizer, evaluate=True)\n","        if training_args.do_eval\n","        else None\n","    )\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer,\n","        mlm=data_args.mlm,\n","    )\n","\n","    # Initialize our Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        data_collator=data_collator,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        prediction_loss_only=True,\n","    )\n","\n","    # Training\n","    try:\n","      if training_args.do_train:\n","          model_path = (\n","              model_args.model_name_or_path\n","              if model_args.model_name_or_path is not None\n","              and os.path.isdir(model_args.model_name_or_path)\n","              else None\n","          )\n","          trainer.train(model_path=model_path)\n","          trainer.save_model()\n","          tokenizer.save_pretrained(training_args.output_dir)\n","    except KeyboardInterrupt:\n","      print(\"Saving model that was in the middle of training\")\n","      trainer.save_model()\n","      tokenizer.save_pretrained(training_args.output_dir)\n","      return\n","\n","    # Evaluation\n","    results = {}\n","    if training_args.do_eval:\n","        logger.info(\"*** Evaluate ***\")\n","\n","        eval_output = trainer.evaluate()\n","\n","        perplexity = math.exp(eval_output[\"eval_loss\"])\n","        result = {\"perplexity\": perplexity}\n","\n","        output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n","        if trainer.is_world_master():\n","            with open(output_eval_file, \"w\") as writer:\n","                logger.info(\"***** Eval results *****\")\n","                for key in sorted(result.keys()):\n","                    logger.info(\"  %s = %s\", key, str(result[key]))\n","                    writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","\n","        results.update(result)\n","\n","    return results\n"],"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"lX2E71vZz385","outputId":"33ac277d-2074-40bd-ca0d-48d67ec62c22"},"source":["main()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["11/15/2021 08:56:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","11/15/2021 08:56:08 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.EPOCH,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint/runs/Nov15_08-56-08_19f1934938df,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3,\n","output_dir=/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=4,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint,\n","save_on_each_node=False,\n","save_steps=1000,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=1,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"id":"RdzdlR4ciaZ6","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"133802c0-7656-4a45-fe99-4ce8c0f69fad"},"source":["# Press the Run Cell button to the left to start training\n","if __name__ == \"__main__\":\n","    main()\n","# To stop training and save model, press the same Run Cell button (now, it is the Interrupt Execution button)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["11/15/2021 08:46:35 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","11/15/2021 08:46:35 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.EPOCH,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint/runs/Nov15_08-46-35_19f1934938df,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3,\n","output_dir=/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=4,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint,\n","save_on_each_node=False,\n","save_steps=1000,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=1,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"id":"xQ4GPsSfQrDH"},"source":["# This cell is to style the Google Colab's output properly (Just blindly run this)\n","from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUoTHvmHHz43","colab":{"base_uri":"https://localhost:8080/","height":562},"executionInfo":{"status":"error","timestamp":1636966594330,"user_tz":-60,"elapsed":492,"user":{"displayName":"charlene","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12236786138296271661"}},"outputId":"73599fe9-53f2-4e39-9fc8-fbca3267b4a2"},"source":["# Run these cells for story generation\n","from transformers import pipeline, TextGenerationPipeline, GPT2LMHeadModel, AutoTokenizer\n","\n","\"\"\" Ci-dessous, mon point de contr√¥le mod√®le est comment√©. Vous pouvez remplacer votre point de contr√¥le\n","avec cela pour tester la g√©n√©ration d'histoires si votre point de contr√¥le ne s'est pas entra√Æn√© assez longtemps\n","\"\"\"\n","#checkpoint = \"pranavpsv/gpt2-genre-story-generator\"\n","checkpoint = \"/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint\"\n","\n","model = GPT2LMHeadModel.from_pretrained(checkpoint)\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","story_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n","# The format for input_prompt: \"<BOS> <genre> Optional text...\"\n","# Supported genres: superhero, sci_fi, horror, thriller, action, drama"],"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["404 Client Error: Not Found for url: https://huggingface.co//content/drive/My%20Drive/projet-TIA/checkpoint/story_generator_checkpoint/resolve/main/config.json\n"]},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1662\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1663\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1664\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co//content/drive/My%20Drive/projet-TIA/checkpoint/story_generator_checkpoint/resolve/main/config.json","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-67a28ccc5667>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#checkpoint = \"/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mstory_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextGenerationPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m                 \u001b[0m_from_auto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_auto_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m                 \u001b[0m_from_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_pipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m             )\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \"\"\"\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             logger.warn(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"- or '{revision}' is a valid git identifier (branch name, a tag name, or a commit id) that exists for this model name as listed on its model page on 'https://huggingface.co/models'\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Can't load config for '/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint'. Make sure that:\n\n- '/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure '/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint' is not a path to a local directory with something else, in that case)\n\n- or '/content/drive/My Drive/projet-TIA/checkpoint/story_generator_checkpoint' is the correct path to a directory containing a config.json file\n\n"]}]},{"cell_type":"code","metadata":{"id":"clXP7bNrPxja","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1598608678496,"user_tz":-120,"elapsed":6306,"user":{"displayName":"Pranav Vadrevu","photoUrl":"https://lh6.googleusercontent.com/-STjRKi70QFE/AAAAAAAAAAI/AAAAAAAACK4/8tBv3gsZ-Ro/s64/photo.jpg","userId":"00326039388269113607"}},"outputId":"cee1e04d-e13b-44ba-b096-7c1d91362ce7"},"source":["input_prompt = \"<BOS> <horror>\"\n","story = story_generator(input_prompt, max_length=75, do_sample=True,\n","               repetition_penalty=1.1, temperature=1.2, \n","               top_p=0.95, top_k=50)\n","print(story)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["08/28/2020 09:57:52 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"],"name":"stderr"},{"output_type":"stream","text":["[{'generated_text': '<BOS> <horror> Following a series of murders, paranormal researchers, led by Alex Riedlke (Bela Lugosi) believe there is a connection between the murders and paranormal activity in their community. Their research reveals a strange dark cycle that will keep on happening as people are killed by a strange force.  Riedlke and his assistants discover a secret society behind the bar'}]\n"],"name":"stdout"}]}]}